{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kairi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\kairi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import collections\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the words pf just a selection of documents (austen-emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\kairi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Task 1 (2 marks)\n",
    "\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "def count_pos(document, pos):\n",
    "    \"\"\"Return the number of occurrences of words with a given part of speech. To find the part of speech, use \n",
    "    NLTK's \"Universal\" tag set. To find the words of the document, use NLTK's sent_tokenize and word_tokenize.\n",
    "    >>> count_pos('austen-emma.txt', 'NOUN')\n",
    "    31998\n",
    "    >>> count_pos('austen-sense.txt', 'VERB')\n",
    "    25074\"\"\"\n",
    "\n",
    "    guten_sents = [nltk.word_tokenize(s) for s in nltk.sent_tokenize(nltk.corpus.gutenberg.raw(document))]\n",
    "    tagged_guten_sents = nltk.pos_tag_sents(guten_sents, tagset=\"universal\")\n",
    "\n",
    "    count = 0 \n",
    "    for s in tagged_guten_sents:\n",
    "        for w in s:\n",
    "          x = w[1]\n",
    "          if x == pos:\n",
    "            count+=1\n",
    "    return count    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pos('austen-emma.txt', 'NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25074"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pos('austen-sense.txt', 'VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 (2 marks)\n",
    "def get_top_stem_bigrams(document, n):\n",
    "    \"\"\"Return the n most frequent bigrams of stems. Return the list sorted in descending order of frequency.\n",
    "    The stems of words in different sentences cannot form a bigram. To stem a word, use NLTK's Porter stemmer.\n",
    "    To find the words of the document, use NLTK's sent_tokenize and word_tokenize.\n",
    "    >>> get_top_stem_bigrams('austen-emma.txt', 3)\n",
    "    [(',', 'and'), ('.', \"''\"), (';', 'and')]\n",
    "    >>> get_top_stem_bigrams('austen-sense.txt',4)\n",
    "    [(',', 'and'), ('.', \"''\"), (';', 'and'), (',', \"''\")]\n",
    "    \"\"\"\n",
    "    sent_tokens =[nltk.word_tokenize(s) for s in nltk.sent_tokenize(nltk.corpus.gutenberg.raw(document))]\n",
    "    s = nltk.PorterStemmer()\n",
    "    dic_stem=[[s.stem(d) for d in sents]for sents in sent_tokens]\n",
    "\n",
    "    bigrams = []\n",
    "    for s in dic_stem:\n",
    "        bigrams += nltk.bigrams([p for p in s])\n",
    "\n",
    "    c = collections.Counter(bigrams)\n",
    "    return [b for b, f in c.most_common(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'and'), ('.', \"''\"), (';', 'and')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_stem_bigrams('austen-emma.txt', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be [(',', 'and'), ('.', \"''\"), (';', 'and')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 'and'), ('.', \"''\"), (';', 'and'), (',', \"''\")]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_stem_bigrams('austen-sense.txt',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sould be  [(',', 'and'), ('.', \"''\"), (';', 'and'), (',', \"''\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 (2 marks)\n",
    "def get_same_stem(document, word):\n",
    "    \"\"\"Return the list of words that have the same stem as the word given, and their frequencies. \n",
    "    To find the stem, use NLTK's Porter stemmer. To find the words of the document, use NLTK's \n",
    "    sent_tokenize and word_tokenize. The resulting list must be sorted alphabetically.\n",
    "\n",
    "    \n",
    "    >>> get_same_stem('austen-emma.txt','respect')[:5]\n",
    "    [('Respect', 2), ('respect', 41), ('respectability', 1), ('respectable', 20), ('respectably', 1)]\n",
    "    >>> get_same_stem('austen-sense.txt','respect')[:5]\n",
    "    [('respect', 22), ('respectability', 1), ('respectable', 14), ('respectably', 1), ('respected', 3)]\n",
    "    \"\"\"\n",
    "    words =nltk.word_tokenize(nltk.corpus.gutenberg.raw(document))\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    word_stem = stemmer.stem(word)\n",
    "    \n",
    "    \n",
    "    same_stem_list = []\n",
    "    for w in words:\n",
    "        if stemmer.stem(w) == word_stem and (w, words.count(w)) not in same_stem_list:\n",
    "            same_stem_list.append((w, words.count(w)))\n",
    "    return sorted(same_stem_list, key=lambda x: x[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Respect', 2),\n",
       " ('respect', 41),\n",
       " ('respectability', 1),\n",
       " ('respectable', 20),\n",
       " ('respectably', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_same_stem('austen-emma.txt','respect')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be [('Respect', 2), ('respect', 41), ('respectability', 1), ('respectable', 20), ('respectably', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('respect', 22),\n",
       " ('respectability', 1),\n",
       " ('respectable', 14),\n",
       " ('respectably', 1),\n",
       " ('respected', 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_same_stem('austen-sense.txt','respect')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be[('respect', 22), ('respectability', 1), ('respectable', 14), ('respectably', 1), ('respected', 3)]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 (2 marks)\n",
    "def most_frequent_after_pos(document, pos):\n",
    "    \"\"\"Return the most frequent word after a given part of speech, and its frequency. Do not consider words\n",
    "    that occur in the next sentence after the given part of speech.\n",
    "    To find the part of speech, use NLTK's \"Universal\" tagset.\n",
    "    >>> most_frequent_after_pos('austen-emma.txt','VERB')\n",
    "    [('not', 1932)]\n",
    "    >>> most_frequent_after_pos('austen-sense.txt','NOUN')\n",
    "    [(',', 5310)]\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = [nltk.word_tokenize(s) for s in nltk.sent_tokenize(nltk.corpus.gutenberg.raw(document))]\n",
    "    tagged_sents = nltk.pos_tag_sents(sents, tagset=\"universal\")\n",
    "\n",
    "    \n",
    "    filtered_pos = []\n",
    "    for s in tagged_sents:\n",
    "        bigrams = nltk.bigrams(s)\n",
    "        filtered_pos += [w2 for (w1, p1), (w2, p2) in bigrams if p1 == pos]\n",
    "    c = collections.Counter(filtered_pos)\n",
    "    return(c.most_common(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', 1932)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_after_pos('austen-emma.txt','VERB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 5310)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_frequent_after_pos('austen-sense.txt','NOUN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5 (2 marks)\n",
    "def get_word_tfidf(text):\n",
    "    \"\"\"Return the tf.idf of the words given in the text. If a word does not have tf.idf information or is zero, \n",
    "    then do not return its tf.idf. The reference for computing tf.idf is the list of documents from the NLTK \n",
    "    Gutenberg corpus. To compute the tf.idf, use sklearn's TfidfVectorizer with the option to remove the English \n",
    "    stop words (stop_words='english'). The result must be a list of words sorted in alphabetical order, together \n",
    "    with their tf.idf.\n",
    "    >>> get_word_tfidf('Emma is a respectable person')\n",
    "    [('emma', 0.8310852062844262), ('person', 0.3245184217533661), ('respectable', 0.4516471784898886)]\n",
    "    >>> get_word_tfidf('Brutus is a honourable person')\n",
    "    [('brutus', 0.8405129362379974), ('honourable', 0.4310718596448824), ('person', 0.32819971943754456)]\n",
    "    \"\"\"\n",
    "    tfidf = TfidfVectorizer(input ='content', stop_words='english')\n",
    "    data = [nltk.corpus.gutenberg.raw(f) for f in nltk.corpus.gutenberg.raw(text)]\n",
    "    tfidf.fit(data)\n",
    "\n",
    "    text = 'Brutus is a honourable person'\n",
    "    text = [text]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5188/1574429875.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_word_tfidf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Emma is a respectable person'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5188/3271546875.py\u001b[0m in \u001b[0;36mget_word_tfidf\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m     13\u001b[0m     \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'document' is not defined"
     ]
    }
   ],
   "source": [
    "get_word_tfidf('Emma is a respectable person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tfidf('Brutus is a honourable person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 8, in __main__.count_pos\n",
      "Failed example:\n",
      "    count_pos('austen-emma.txt', 'NOUN')\n",
      "Expected:\n",
      "    31998\n",
      "Got:\n",
      "    32000\n",
      "**********************************************************************\n",
      "File \"__main__\", line 8, in __main__.get_word_tfidf\n",
      "Failed example:\n",
      "    get_word_tfidf('Emma is a respectable person')\n",
      "Expected:\n",
      "    [('emma', 0.8310852062844262), ('person', 0.3245184217533661), ('respectable', 0.4516471784898886)]\n",
      "Got:\n",
      "    [('emma', 1.0)]\n",
      "**********************************************************************\n",
      "File \"__main__\", line 10, in __main__.get_word_tfidf\n",
      "Failed example:\n",
      "    get_word_tfidf('Brutus is a honourable person')\n",
      "Expected:\n",
      "    [('brutus', 0.8405129362379974), ('honourable', 0.4310718596448824), ('person', 0.32819971943754456)]\n",
      "Got:\n",
      "    []\n",
      "**********************************************************************\n",
      "2 items had failures:\n",
      "   1 of   2 in __main__.count_pos\n",
      "   2 of   2 in __main__.get_word_tfidf\n",
      "***Test Failed*** 3 failures.\n"
     ]
    }
   ],
   "source": [
    "# DO NOT MODIFY THE CODE BELOW\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod(optionflags=doctest.ELLIPSIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ac64aa34b5f865596d01c8cac47869d40725a671b91407f6b8a3f5346e33778"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
